{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd2f8e99",
   "metadata": {
    "papermill": {
     "duration": 0.00259,
     "end_time": "2025-12-14T12:21:59.638386",
     "exception": false,
     "start_time": "2025-12-14T12:21:59.635796",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Kaggle Submission – TabPFN 单体版\n",
    "\n",
    "该 Notebook 完全使用 TabPFN，不依赖项目内模块文件。每个代码单元封装为相对独立的“模块”，便于后续拆分成独立 py 文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2ad4003",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T12:21:59.643448Z",
     "iopub.status.busy": "2025-12-14T12:21:59.643227Z",
     "iopub.status.idle": "2025-12-14T12:22:00.975180Z",
     "shell.execute_reply": "2025-12-14T12:22:00.974615Z"
    },
    "papermill": {
     "duration": 1.33583,
     "end_time": "2025-12-14T12:22:00.976480",
     "exception": false,
     "start_time": "2025-12-14T12:21:59.640650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 0) 环境设置与路径管理\n",
    "# 从 __future__ 导入 annotations，确保类型提示的向前兼容性\n",
    "from __future__ import annotations\n",
    "# 导入数据类，用于创建结构化的数据容器\n",
    "from dataclasses import dataclass\n",
    "# 导入 pathlib，用于面向对象的文件系统路径操作\n",
    "from pathlib import Path\n",
    "# 导入 typing 中的高级类型提示\n",
    "from typing import Optional, Dict, List, Sequence, Tuple\n",
    "# 导入 os 模块，用于与操作系统交互，如此处设置环境变量\n",
    "import os\n",
    "# 导入 json 模块，用于处理 JSON 数据\n",
    "import json\n",
    "# 导入 numpy，用于高效的数值计算\n",
    "import numpy as np\n",
    "# 导入 pandas，用于数据处理和分析\n",
    "import pandas as pd\n",
    "\n",
    "# --- 全局常量和环境配置 ---\n",
    "\n",
    "# 检测是否在 Kaggle 环境中运行，Kaggle 的工作目录是 /kaggle/working\n",
    "IS_KAGGLE = Path('/kaggle/working').exists()\n",
    "# 获取当前工作目录\n",
    "CWD = Path.cwd()\n",
    "# 解析输入数据的基础路径。在 Kaggle 环境中，路径通常是 ../input/\n",
    "BASE_INPUT = (CWD.parent / \"input\") if CWD.name == \"working\" else (Path(\"..\") / \"input\")\n",
    "# 定义 Hugging Face 模型缓存的路径，指向预先下载好的数据集\n",
    "HF_CACHE = BASE_INPUT / \"package-hf-cache-tabpfn\" / \"hf_cache\"\n",
    "# TabPFN 模型的本地检查点文件路径\n",
    "LOCAL_CKPT = HF_CACHE / \"tabpfn-v2-regressor.ckpt\"\n",
    "# 设置 Hugging Face 的环境变量，使其使用本地缓存\n",
    "os.environ.setdefault(\"HF_HOME\", str(HF_CACHE))\n",
    "# 强制 Hugging Face Hub 使用离线模式，避免在 Kaggle Notebook 中联网下载\n",
    "os.environ.setdefault(\"HF_HUB_OFFLINE\", \"1\")\n",
    "\n",
    "# 使用 dataclass 定义一个用于存储所有重要路径的容器\n",
    "@dataclass\n",
    "class Paths:\n",
    "    data_root: Path          # 数据集根目录\n",
    "    train_csv: Path          # 训练数据 CSV 文件路径\n",
    "    test_csv: Path           # 测试数据 CSV 文件路径\n",
    "    out_dir: Path            # 输出目录\n",
    "    model_dir: Path          # 模型保存目录\n",
    "    submission_csv: Path     # 提交文件 (CSV 格式) 路径\n",
    "    submission_parquet: Path # 提交文件 (Parquet 格式) 路径\n",
    "    metadata_path: Path      # 模型元数据文件路径\n",
    "\n",
    "# 定义一个函数来解析并构建所有需要的路径\n",
    "def resolve_paths(verbose: bool = True) -> Paths:\n",
    "    # 比赛数据的根目录\n",
    "    data_root = (BASE_INPUT / \"hull-tactical-market-prediction\").resolve()\n",
    "    # 输出目录设置为当前工作目录\n",
    "    out_dir = CWD.resolve()\n",
    "    # 模型目录设置在输出目录下\n",
    "    model_dir = out_dir / \"tabpfn_model\"\n",
    "    # 创建并填充 Paths 对象\n",
    "    paths = Paths(\n",
    "        data_root=data_root,\n",
    "        train_csv=data_root / \"train.csv\",\n",
    "        test_csv=data_root / \"test.csv\",\n",
    "        out_dir=out_dir,\n",
    "        model_dir=model_dir,\n",
    "        submission_csv=out_dir / \"submission.csv\",\n",
    "        submission_parquet=out_dir / \"submission.parquet\",\n",
    "        metadata_path=model_dir / \"metadata.json\",\n",
    "    )\n",
    "    # 如果 verbose 为 True，则打印所有解析出的路径，方便调试\n",
    "    if verbose:\n",
    "        print(\"=== Paths ===\")\n",
    "        for k, v in paths.__dict__.items():\n",
    "            print(f\"{k:18s}: {v}\")\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a57a5eaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T12:22:00.982219Z",
     "iopub.status.busy": "2025-12-14T12:22:00.981536Z",
     "iopub.status.idle": "2025-12-14T12:22:07.586167Z",
     "shell.execute_reply": "2025-12-14T12:22:07.585384Z"
    },
    "papermill": {
     "duration": 6.608864,
     "end_time": "2025-12-14T12:22:07.587536",
     "exception": false,
     "start_time": "2025-12-14T12:22:00.978672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wheel dir: /kaggle/input/kaggle-linux-wheels-tabpfn\n",
      "提示：本赛提交需要 /kaggle/working/submission.parquet；请不要在 Kaggle 里 pip 安装/升级 pyarrow。\n",
      "Looking in links: /kaggle/input/kaggle-linux-wheels-tabpfn\n",
      "Processing /kaggle/input/kaggle-linux-wheels-tabpfn/backoff-2.2.1-py3-none-any.whl\n",
      "Requirement already satisfied: distro==1.9.0 in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
      "Installing collected packages: backoff\n",
      "Successfully installed backoff-2.2.1\n",
      "Looking in links: /kaggle/input/kaggle-linux-wheels-tabpfn\n",
      "Processing /kaggle/input/kaggle-linux-wheels-tabpfn/posthog-6.7.4-py3-none-any.whl\n",
      "Installing collected packages: posthog\n",
      "Successfully installed posthog-6.7.4\n",
      "Looking in links: /kaggle/input/kaggle-linux-wheels-tabpfn\n",
      "Processing /kaggle/input/kaggle-linux-wheels-tabpfn/tabpfn_common_utils-0.2.10-py3-none-any.whl\n",
      "Processing /kaggle/input/kaggle-linux-wheels-tabpfn/kditransform-1.2.0-py3-none-any.whl\n",
      "Processing /kaggle/input/kaggle-linux-wheels-tabpfn/eval_type_backport-0.3.1-py3-none-any.whl\n",
      "Installing collected packages: kditransform, tabpfn-common-utils, eval-type-backport\n",
      "Successfully installed eval-type-backport-0.3.1 kditransform-1.2.0 tabpfn-common-utils-0.2.10\n",
      "Looking in links: /kaggle/input/kaggle-linux-wheels-tabpfn\n",
      "Processing /kaggle/input/kaggle-linux-wheels-tabpfn/tabpfn-6.0.6-py3-none-any.whl\n",
      "Installing collected packages: tabpfn\n",
      "Successfully installed tabpfn-6.0.6\n"
     ]
    }
   ],
   "source": [
    "# 若在 Kaggle 环境中运行，则按需使用离线轮子（Kaggle 自带 GPU 版 torch）\n",
    "if IS_KAGGLE:\n",
    "    import os\n",
    "    import subprocess\n",
    "    from pathlib import Path\n",
    "\n",
    "    wheel_ds = os.environ.get(\"TABPFN_WHEEL_DATASET\", \"kaggle-linux-wheels-tabpfn\")\n",
    "    wheel_dir = Path(f\"/kaggle/input/{wheel_ds}\")\n",
    "    if (wheel_dir / \"kaggle-linux-wheels-tabpfn\").exists():\n",
    "        wheel_dir = wheel_dir / \"kaggle-linux-wheels-tabpfn\"\n",
    "    print(\"Wheel dir:\", wheel_dir)\n",
    "    print(\"提示：本赛提交需要 /kaggle/working/submission.parquet；请不要在 Kaggle 里 pip 安装/升级 pyarrow。\")\n",
    "\n",
    "    def pip_install(*packages: str, no_deps: bool = False) -> None:\n",
    "        cmd = [\"pip\", \"install\", \"--no-index\", f\"--find-links={wheel_dir}\"]\n",
    "        if no_deps:\n",
    "            cmd.append(\"--no-deps\")\n",
    "        cmd += list(packages)\n",
    "        subprocess.check_call(cmd)\n",
    "\n",
    "    # 关键：用 --no-deps 避免 pip 去解析/改动 torch 与 nvidia-*（Kaggle 已自带 GPU torch）\n",
    "    pip_install(\"backoff==2.2.1\", \"distro==1.9.0\", no_deps=True)\n",
    "    pip_install(\"posthog==6.7.4\", no_deps=True)\n",
    "    pip_install(\"tabpfn-common-utils==0.2.10\", \"kditransform==1.2.0\", \"eval-type-backport==0.3.1\", no_deps=True)\n",
    "    pip_install(\"tabpfn==6.0.6\", no_deps=True)\n",
    "\n",
    "    os.environ.setdefault(\"HF_HOME\", str(wheel_dir))\n",
    "    os.environ.setdefault(\"HF_HUB_OFFLINE\", \"1\")\n",
    "    os.environ.setdefault(\"TABPFN_MODEL_PATH\", str(wheel_dir / \"tabpfn-v2-regressor.ckpt\"))\n",
    "else:\n",
    "    print(\"Not Kaggle environment; skip online install block\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.5) ?????RAM / VRAM?\n",
    "# ?????????? log_resources(\"tag\")?????????????????\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    import psutil  # ????????\n",
    "except Exception:\n",
    "    psutil = None\n",
    "\n",
    "_RESOURCE_LAST_T = 0.0\n",
    "\n",
    "def _rss_mb() -> float | None:\n",
    "    # Prefer psutil\n",
    "    try:\n",
    "        if psutil is not None:\n",
    "            return psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Linux fallback (/proc)\n",
    "    try:\n",
    "        with open('/proc/self/status', 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('VmRSS:'):\n",
    "                    return int(line.split()[1]) / 1024\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return None\n",
    "\n",
    "def log_resources(tag: str = '', *, force: bool = False, min_interval_s: float | None = None, include_nvidia_smi: bool = False) -> None:\n",
    "    \"\"\"Print RAM/VRAM usage with rate limiting.\n",
    "\n",
    "    - force=True: always print\n",
    "    - min_interval_s: override env LOG_RESOURCES_MIN_INTERVAL_S (default 10s)\n",
    "    - include_nvidia_smi=True: also query nvidia-smi (slower)\n",
    "    \"\"\"\n",
    "    global _RESOURCE_LAST_T\n",
    "\n",
    "    if os.environ.get('LOG_RESOURCES', '1') in ('0', 'false', 'False'):\n",
    "        return\n",
    "\n",
    "    now = time.monotonic()\n",
    "    if not force:\n",
    "        if min_interval_s is None:\n",
    "            try:\n",
    "                min_interval_s = float(os.environ.get('LOG_RESOURCES_MIN_INTERVAL_S', '10'))\n",
    "            except Exception:\n",
    "                min_interval_s = 10.0\n",
    "        if now - _RESOURCE_LAST_T < float(min_interval_s):\n",
    "            return\n",
    "    _RESOURCE_LAST_T = now\n",
    "\n",
    "    parts = [f\"[RES] {tag}\".strip()]\n",
    "\n",
    "    rss = _rss_mb()\n",
    "    if rss is not None:\n",
    "        parts.append(f\"RSS={rss:.0f}MB\")\n",
    "\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            free, total = torch.cuda.mem_get_info()\n",
    "            parts.append(f\"VRAM_free={free/1024**3:.2f}GB\")\n",
    "            parts.append(f\"VRAM_total={total/1024**3:.2f}GB\")\n",
    "            parts.append(f\"alloc={torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "            parts.append(f\"reserved={torch.cuda.memory_reserved()/1024**3:.2f}GB\")\n",
    "            try:\n",
    "                parts.append(f\"max_alloc={torch.cuda.max_memory_allocated()/1024**3:.2f}GB\")\n",
    "            except Exception:\n",
    "                pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if include_nvidia_smi and shutil.which('nvidia-smi'):\n",
    "        try:\n",
    "            out = subprocess.check_output(\n",
    "                ['nvidia-smi', '--query-gpu=memory.used,memory.total', '--format=csv,noheader,nounits'],\n",
    "                text=True,\n",
    "            ).strip()\n",
    "            parts.append(f\"nvidia-smi(MB)={out}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    print(' | '.join(parts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6b41e0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T12:22:07.593673Z",
     "iopub.status.busy": "2025-12-14T12:22:07.593144Z",
     "iopub.status.idle": "2025-12-14T12:22:07.599235Z",
     "shell.execute_reply": "2025-12-14T12:22:07.598719Z"
    },
    "papermill": {
     "duration": 0.010282,
     "end_time": "2025-12-14T12:22:07.600314",
     "exception": false,
     "start_time": "2025-12-14T12:22:07.590032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1) 数据加载与预处理\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# 定义一个数据容器，用于捆绑加载后的训练集、测试集和重叠日期信息\n",
    "@dataclass\n",
    "class DataBundle:\n",
    "    train: pd.DataFrame\n",
    "    test: pd.DataFrame\n",
    "    overlap_dates: List[int]\n",
    "\n",
    "# 定义加载数据的函数\n",
    "def load_data(paths: Paths) -> DataBundle:\n",
    "    # 使用 pandas 从 CSV 文件加载训练和测试数据\n",
    "    train_df = pd.read_csv(paths.train_csv)\n",
    "    test_df = pd.read_csv(paths.test_csv)\n",
    "\n",
    "    # --- 处理时间序列泄漏 ---\n",
    "    # 竞赛说明中提到，训练集包含一部分与测试集时间重叠的数据（数据泄漏）。\n",
    "    # 为了建立一个稳健的模型，我们需要识别并移除这部分数据。\n",
    "\n",
    "    # 获取测试集的最早日期 ID\n",
    "    test_start = int(test_df[\"date_id\"].min())\n",
    "    # 创建一个布尔掩码，标记出训练集中日期大于等于测试集起始日期的行\n",
    "    leak_mask = train_df[\"date_id\"] >= test_start\n",
    "    # 提取这些重叠的、唯一的日期 ID\n",
    "    overlap_dates = sorted(train_df.loc[leak_mask, \"date_id\"].unique().tolist())\n",
    "\n",
    "    # 从训练集中移除存在泄漏的行\n",
    "    cleaned_train = (\n",
    "        train_df.loc[~leak_mask]\n",
    "        .sort_values(\"date_id\") # 按日期排序\n",
    "        .reset_index(drop=True) # 重置索引\n",
    "    )\n",
    "    # 对测试集也进行排序和索引重置，以确保数据一致性\n",
    "    cleaned_test = test_df.sort_values(\"date_id\").reset_index(drop=True)\n",
    "\n",
    "    # 打印清洗后数据的信息\n",
    "    print(f\"Train rows: {len(cleaned_train)} | Test rows: {len(cleaned_test)} | Overlap dates: {len(overlap_dates)}\")\n",
    "    # 返回包含清洗后数据的 DataBundle 对象\n",
    "    return DataBundle(train=cleaned_train, test=cleaned_test, overlap_dates=overlap_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b0fe166",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T12:22:07.605815Z",
     "iopub.status.busy": "2025-12-14T12:22:07.605601Z",
     "iopub.status.idle": "2025-12-14T12:22:07.615309Z",
     "shell.execute_reply": "2025-12-14T12:22:07.614824Z"
    },
    "papermill": {
     "duration": 0.013751,
     "end_time": "2025-12-14T12:22:07.616385",
     "exception": false,
     "start_time": "2025-12-14T12:22:07.602634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2) 特征工程\n",
    "\n",
    "# --- 常量定义 ---\n",
    "# 目标变量列名\n",
    "TARGET_COL = \"forward_returns\"\n",
    "\n",
    "# 用于创建滞后特征的源列。这些是时间序列相关的关键指标。\n",
    "LAG_SOURCE_COLUMNS: Tuple[str, ...] = (\n",
    "    \"forward_returns\",                 # 未来收益率\n",
    "    \"market_forward_excess_returns\",   # 市场未来超额收益率\n",
    "    \"risk_free_rate\",                  # 无风险利率\n",
    ")\n",
    "\n",
    "# 用于识别\"前瞻性\"列的关键词。这些列包含未来信息，在训练时不能作为特征使用，否则会导致数据泄漏。\n",
    "FORWARD_KEYWORDS: Tuple[str, ...] = (\"forward\", \"future\", \"lead\", \"next\")\n",
    "# 安全的前缀。如果列名以这些前缀开头（例如 'lagged_forward_returns'），则不认为它是前瞻性的。\n",
    "SAFE_FORWARD_PREFIXES: Tuple[str, ...] = (\"lagged_\",)\n",
    "# 需要从特征集中明确排除的列。这些列要么是目标变量，要么是目标的变体，或不应作为特征。\n",
    "EXCLUDE_COLUMNS: Tuple[str, ...] = (\n",
    "    \"forward_returns\",\n",
    "    \"risk_free_rate\",\n",
    "    \"market_forward_excess_returns\",\n",
    ")\n",
    "\n",
    "# 辅助函数，判断一个列名是否表示前瞻性信息\n",
    "def _is_forward_looking(name: str) -> bool:\n",
    "    lower = name.lower()\n",
    "    # 如果列名以安全前缀开头，则不是前瞻性的\n",
    "    if any(lower.startswith(prefix) for prefix in SAFE_FORWARD_PREFIXES):\n",
    "        return False\n",
    "    # 如果列名包含任何前瞻性关键词，则认为是前瞻性的\n",
    "    return any(keyword in lower for keyword in FORWARD_KEYWORDS)\n",
    "\n",
    "# 定义一个数据容器，用于捆绑所有与特征相关的 DataFrame\n",
    "@dataclass\n",
    "class FeatureBundle:\n",
    "    feature_columns: List[str]      # 特征列名列表\n",
    "    train_features: pd.DataFrame   # 训练特征集 (X_train)\n",
    "    test_features: pd.DataFrame    # 测试特征集 (X_test)\n",
    "    train_with_target: pd.DataFrame # 带有目标列的训练集 (用于 TabPFN)\n",
    "    calibration_frame: pd.DataFrame # 用于后续 Sharpe 校准的数据\n",
    "\n",
    "# 主函数，构建所有特征\n",
    "def build_features(bundle: DataBundle) -> FeatureBundle:\n",
    "    train_df = bundle.train.copy()\n",
    "    test_df = bundle.test.copy()\n",
    "\n",
    "    # --- 创建滞后特征 ---\n",
    "    # 移位操作 (shift) 会将前一天的数据作为当前行的新特征，这是处理时间序列数据的常用方法。\n",
    "    for col in LAG_SOURCE_COLUMNS:\n",
    "        train_df[f\"lagged_{col}\"] = train_df[col].shift(1)\n",
    "\n",
    "    # 由于 shift(1) 会在第一行产生 NaN，需要删除这些行\n",
    "    lag_columns = [f\"lagged_{c}\" for c in LAG_SOURCE_COLUMNS]\n",
    "    train_df = train_df.dropna(subset=lag_columns).reset_index(drop=True)\n",
    "\n",
    "    # --- 筛选特征列 ---\n",
    "    base_exclude = set(EXCLUDE_COLUMNS)\n",
    "    feature_columns = [\n",
    "        col\n",
    "        for col in train_df.columns\n",
    "        # 确保特征在测试集中也存在\n",
    "        if col in test_df.columns\n",
    "        # 排除预定义的列\n",
    "        and col not in base_exclude\n",
    "        # 排除所有前瞻性的列\n",
    "        and not _is_forward_looking(col)\n",
    "    ]\n",
    "    feature_columns.sort() # 对列名排序，确保每次运行的顺序一致\n",
    "\n",
    "    # --- 准备最终的 DataFrame ---\n",
    "    # 使用 reindex 确保训练集和测试集有完全相同的列序，并用 0 填充缺失值\n",
    "    train_features = train_df.reindex(columns=feature_columns, fill_value=np.nan).fillna(0)\n",
    "    test_features = test_df.reindex(columns=feature_columns, fill_value=np.nan).fillna(0)\n",
    "    # 创建一个包含特征和目标列的 DataFrame，这是 TabPFN.fit() 方法需要的格式\n",
    "    train_with_target = train_features.copy()\n",
    "    train_with_target[TARGET_COL] = train_df[TARGET_COL].to_numpy(dtype=np.float64)\n",
    "    # 创建一个用于校准的 DataFrame，只包含必要列\n",
    "    calibration_frame = train_df[[\"date_id\", \"forward_returns\", \"risk_free_rate\"]].copy()\n",
    "\n",
    "    print(f\"Feature columns: {len(feature_columns)}\")\n",
    "    return FeatureBundle(\n",
    "        feature_columns=feature_columns,\n",
    "        train_features=train_features,\n",
    "        test_features=test_features,\n",
    "        train_with_target=train_with_target,\n",
    "        calibration_frame=calibration_frame,\n",
    "    )\n",
    "\n",
    "# 辅助函数，用于在预测时准备特征。确保应用与训练时相同的列和填充逻辑。\n",
    "def prepare_features(df: pd.DataFrame, feature_columns: Sequence[str]) -> pd.DataFrame:\n",
    "    return df.reindex(columns=feature_columns, fill_value=np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd2f99b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T12:22:07.621770Z",
     "iopub.status.busy": "2025-12-14T12:22:07.621576Z",
     "iopub.status.idle": "2025-12-14T12:22:08.237121Z",
     "shell.execute_reply": "2025-12-14T12:22:08.236541Z"
    },
    "papermill": {
     "duration": 0.619874,
     "end_time": "2025-12-14T12:22:08.238436",
     "exception": false,
     "start_time": "2025-12-14T12:22:07.618562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3) Sharpe Ratio 校准工具\n",
    "# 本模块的目标是找到一个线性变换 (y = scale * x + shift) 应用于模型的原始预测，\n",
    "# 使得变换后的头寸（position）在历史数据上能最大化一个自定义的“调整后夏普比率”。\n",
    "# 这是一种后处理步骤，旨在使模型的输出更符合竞赛的特定评分标准。\n",
    "from dataclasses import dataclass\n",
    "from scipy.optimize import minimize, Bounds # 从 SciPy 导入优化函数和边界类\n",
    "from warnings import warn\n",
    "\n",
    "# 定义一个数据容器，用于存储校准结果\n",
    "@dataclass\n",
    "class CalibrationResult:\n",
    "    scale: float             # 缩放因子\n",
    "    shift: float             # 平移因子\n",
    "    raw_sharpe: float        # 原始预测的夏普比率\n",
    "    adjusted_sharpe: float   # 校准后预测的夏普比率\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        \"\"\"返回一个包含 scale 和 shift 的元组\"\"\"\n",
    "        return (self.scale, self.shift)\n",
    "\n",
    "# 计算调整后夏普比率的函数\n",
    "def adjusted_sharpe(solution: pd.DataFrame, positions: np.ndarray, min_position: float, max_position: float) -> float:\n",
    "    if solution.empty:\n",
    "        return 0.0\n",
    "    df = solution[[\"forward_returns\", \"risk_free_rate\"]].copy()\n",
    "    # 将头寸限制在允许的最小和最大值之间\n",
    "    clipped_positions = np.clip(np.asarray(positions, dtype=np.float64), min_position, max_position)\n",
    "    df[\"position\"] = clipped_positions\n",
    "    # 计算策略的日收益率：一部分是无风险资产，一部分是根据头寸投资于风险资产\n",
    "    strategy_returns = df[\"risk_free_rate\"] * (1 - df[\"position\"]) + df[\"position\"] * df[\"forward_returns\"]\n",
    "    # 计算策略的超额收益率\n",
    "    excess = strategy_returns - df[\"risk_free_rate\"]\n",
    "    if len(excess) == 0: return 0.0\n",
    "    # 计算几何平均超额收益率\n",
    "    cum_excess = float(np.prod(1 + excess))\n",
    "    mean_excess = cum_excess ** (1 / len(df)) - 1\n",
    "    # 计算策略收益率的标准差\n",
    "    std_excess = float(strategy_returns.std(ddof=0))\n",
    "    if std_excess == 0: return 0.0\n",
    "    annual_days = 252 # 年化天数\n",
    "    # 计算标准夏普比率（年化）\n",
    "    sharpe = mean_excess / std_excess * np.sqrt(annual_days)\n",
    "\n",
    "    # --- 计算惩罚项 ---\n",
    "    # 市场超额收益\n",
    "    market_excess = df[\"forward_returns\"] - df[\"risk_free_rate\"]\n",
    "    market_cum = float(np.prod(1 + market_excess))\n",
    "    market_mean = market_cum ** (1 / len(df)) - 1 if len(df) else 0.0\n",
    "    market_std = float(market_excess.std(ddof=0))\n",
    "    # 年化波动率\n",
    "    market_vol = market_std * np.sqrt(annual_days) * 100\n",
    "    strat_vol = std_excess * np.sqrt(annual_days) * 100\n",
    "    # 1. 波动率惩罚：如果策略波动率超过市场波动率的1.2倍，则施加惩罚\n",
    "    excess_vol_penalty = 1 + max(0, strat_vol / market_vol - 1.2) if market_vol > 0 else 1\n",
    "    # 2. 收益差距惩罚：如果策略的平均收益低于市场平均收益，则施加惩罚\n",
    "    return_gap = max(0, (market_mean - mean_excess) * 100 * annual_days)\n",
    "    return_penalty = 1 + (return_gap ** 2) / 100\n",
    "    # 最终分数为夏普比率除以两个惩罚项\n",
    "    score = sharpe / (excess_vol_penalty * return_penalty)\n",
    "    return float(min(score, 1_000_000)) # 返回分数，并设置一个上限\n",
    "\n",
    "# 应用校准参数（scale 和 shift）到预测值上\n",
    "def apply_calibration(values: np.ndarray, params: Tuple[float, float], min_position: float, max_position: float) -> np.ndarray:\n",
    "    scale, shift = params\n",
    "    arr = np.asarray(values, dtype=np.float64)\n",
    "    # 应用线性变换并裁剪结果\n",
    "    return np.clip(arr * scale + shift, min_position, max_position)\n",
    "\n",
    "# 对预测进行校准的主函数\n",
    "def calibrate_predictions(train_frame: pd.DataFrame, base_predictions: pd.Series, window: int, bounds: Bounds, min_position: float, max_position: float) -> CalibrationResult:\n",
    "    if base_predictions is None or len(base_predictions) == 0:\n",
    "        return CalibrationResult(scale=1.0, shift=0.0, raw_sharpe=0.0, adjusted_sharpe=0.0)\n",
    "    work = train_frame[[\"date_id\", \"forward_returns\", \"risk_free_rate\"]].copy()\n",
    "    work[\"model_pred\"] = base_predictions.values\n",
    "    work = work.sort_values(\"date_id\")\n",
    "    # 使用一个滑动窗口（例如最近180天）的数据进行校准\n",
    "    if len(work) > window:\n",
    "        work = work.tail(window)\n",
    "    work = work.set_index(\"date_id\")\n",
    "\n",
    "    # 定义优化目标函数。优化器会尝试找到使这个函数值最小的参数。\n",
    "    # 我们返回负的夏普比率，因为 minimize 求的是最小值。\n",
    "    def objective(params: np.ndarray) -> float:\n",
    "        adjusted = apply_calibration(work[\"model_pred\"].values, tuple(params), min_position, max_position)\n",
    "        return -adjusted_sharpe(work[[\"forward_returns\", \"risk_free_rate\"]], adjusted, min_position, max_position)\n",
    "\n",
    "    initial = np.array([1.0, 0.0]) # 初始猜测值 (scale=1, shift=0)\n",
    "    try:\n",
    "        # 调用 scipy.optimize.minimize 进行优化\n",
    "        result = minimize(\n",
    "            objective,             # 目标函数\n",
    "            x0=initial,            # 初始参数\n",
    "            method=\"Powell\",       # 优化算法\n",
    "            bounds=bounds,         # 参数的边界\n",
    "            options={\"xtol\": 1e-4, \"ftol\": 1e-4, \"maxiter\": 500}, # 优化选项\n",
    "        )\n",
    "        if not result.success:\n",
    "            warn(f\"Calibration did not converge: {result.message}\")\n",
    "            scale, shift = initial\n",
    "        else:\n",
    "            scale, shift = result.x # 获取优化后的参数\n",
    "    except Exception as exc:\n",
    "        warn(f\"Calibration failed, using identity transform: {exc}\")\n",
    "        scale, shift = initial\n",
    "\n",
    "    # 计算校准前后的夏普比率以供参考\n",
    "    adjusted = apply_calibration(work[\"model_pred\"].values, (scale, shift), min_position, max_position)\n",
    "    raw_score = adjusted_sharpe(work[[\"forward_returns\", \"risk_free_rate\"]], work[\"model_pred\"].values, min_position, max_position)\n",
    "    adj_score = adjusted_sharpe(work[[\"forward_returns\", \"risk_free_rate\"]], adjusted, min_position, max_position)\n",
    "    return CalibrationResult(scale=float(scale), shift=float(shift), raw_sharpe=raw_score, adjusted_sharpe=adj_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b59d4d4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T12:22:08.244358Z",
     "iopub.status.busy": "2025-12-14T12:22:08.244066Z",
     "iopub.status.idle": "2025-12-14T12:22:16.129865Z",
     "shell.execute_reply": "2025-12-14T12:22:16.129264Z"
    },
    "papermill": {
     "duration": 7.89019,
     "end_time": "2025-12-14T12:22:16.131103",
     "exception": false,
     "start_time": "2025-12-14T12:22:08.240913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4) TabPFN 模型服务封装\n",
    "# 这个类将 TabPFN 模型的所有操作（训练、加载、预测、校准）封装起来，提供一个简洁的接口。\n",
    "from tabpfn import TabPFNRegressor\n",
    "import joblib # 用于保存和加载 Python 对象（如此处的模型）\n",
    "import torch\n",
    "import os\n",
    "from scipy.optimize import Bounds\n",
    "import numpy as np\n",
    "\n",
    "class TabPFNService:\n",
    "    def __init__(self, paths: Paths, seed: int = 42, min_position: float = 0.0, max_position: float = 2.0, calibration_window: int = 180, max_training_rows: Optional[int] = 20000):\n",
    "        self.paths = paths\n",
    "        self.seed = seed\n",
    "        self.min_position = min_position\n",
    "        self.max_position = max_position\n",
    "        self.calibration_window = calibration_window\n",
    "        self.max_training_rows = max_training_rows  # limit training samples for runtime control\n",
    "        self.model: Optional[TabPFNRegressor] = None # 模型实例\n",
    "        self.feature_columns: List[str] = [] # 特征列名\n",
    "        self.calibration: Optional[CalibrationResult] = None # 校准结果\n",
    "        self.model_source: str = \"unknown\" # 记录模型来源（'trained' 或 'pretrained'）\n",
    "\n",
    "    @property\n",
    "    def device(self) -> str:\n",
    "        \"\"\"自动检测并返回可用的设备（CUDA GPU 或 CPU）\"\"\"\n",
    "        return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    def fit(self, features: FeatureBundle) -> None:\n",
    "        \"\"\"训练 TabPFN 模型并保存到文件\"\"\"\n",
    "        self.paths.model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        # 初始化 TabPFN 回归器\n",
    "        # model_path: 指向本地的 .ckpt 文件\n",
    "        # n_estimators: 类似于集成方法中的树的数量，TabPFN 用它来做多次推理并平均\n",
    "        # ignore_pretraining_limits: TabPFN 预训练时对特征数量（100）和样本数量（1024）有限制，设置此项为 True 可忽略这些限制\n",
    "        model = TabPFNRegressor(model_path=os.environ.get(\"TABPFN_MODEL_PATH\", str(LOCAL_CKPT)), n_estimators=2, device=self.device, random_state=self.seed, ignore_pretraining_limits=True)\n",
    "        # 训练模型\n",
    "        train_X = features.train_features\n",
    "        train_y = features.train_with_target[TARGET_COL]\n",
    "        original_rows = len(train_X)\n",
    "        if self.max_training_rows is not None and original_rows > self.max_training_rows:\n",
    "            rng = np.random.default_rng(self.seed)\n",
    "            idx = rng.choice(original_rows, self.max_training_rows, replace=False)\n",
    "            train_X = train_X.iloc[idx].reset_index(drop=True)\n",
    "            train_y = train_y.iloc[idx].reset_index(drop=True)\n",
    "            print(f\"TabPFN fit: downsampled from {original_rows} -> {len(train_X)} rows for faster training\")\n",
    "        model.fit(train_X, train_y)\n",
    "        # 将训练好的模型和特征列名打包保存\n",
    "        payload = {\"model\": model, \"feature_columns\": features.feature_columns}\n",
    "        joblib.dump(payload, self.paths.model_dir / \"tabpfn_model.joblib\")\n",
    "        # 更新服务实例的状态\n",
    "        self.model = model\n",
    "        self.feature_columns = list(features.feature_columns)\n",
    "        self.model_source = \"trained\"\n",
    "\n",
    "    def load(self) -> bool:\n",
    "        \"\"\"从文件加载预训练的模型\"\"\"\n",
    "        model_path = self.paths.model_dir / \"tabpfn_model.joblib\"\n",
    "        if not model_path.exists():\n",
    "            return False # 如果模型文件不存在，返回 False\n",
    "        saved = joblib.load(model_path)\n",
    "        self.model = saved[\"model\"]\n",
    "        self.feature_columns = list(saved[\"feature_columns\"])\n",
    "        self.model_source = \"pretrained\"\n",
    "        return True\n",
    "\n",
    "    def predict_raw(self, df: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"\n",
    "        [Fix] Manual batching to prevent OOM on Kaggle T4 GPU.\n",
    "        \"\"\"\n",
    "        feats = prepare_features(df, self.feature_columns)\n",
    "\n",
    "        # T4 Safe Batch Size for TabPFN\n",
    "        BATCH_SIZE = 512\n",
    "        preds_list = []\n",
    "        total_rows = len(feats)\n",
    "\n",
    "        if \"log_resources\" in globals():\n",
    "            log_resources(f\"predict_raw start rows={total_rows} batch={BATCH_SIZE}\", force=True)\n",
    "        print(f\"Starting batched prediction: {total_rows} rows, Batch Size: {BATCH_SIZE}\")\n",
    "\n",
    "        for start_idx in range(0, total_rows, BATCH_SIZE):\n",
    "            end_idx = min(start_idx + BATCH_SIZE, total_rows)\n",
    "            batch_df = feats.iloc[start_idx:end_idx]\n",
    "\n",
    "            try:\n",
    "                batch_pred = self.model.predict(batch_df)\n",
    "                preds_list.append(batch_pred)\n",
    "            except Exception as e:\n",
    "                print(f\"!!! Batch {start_idx}-{end_idx} failed: {e}\")\n",
    "                fallback = np.zeros(len(batch_df), dtype=np.float64)\n",
    "                preds_list.append(fallback)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            import gc\n",
    "            gc.collect()\n",
    "\n",
    "            if \"log_resources\" in globals():\n",
    "                log_resources(f\"predict_raw batch {start_idx}-{end_idx}\")\n",
    "\n",
    "        if len(preds_list) > 0:\n",
    "            final_preds = np.concatenate(preds_list)\n",
    "        else:\n",
    "            final_preds = np.array([])\n",
    "\n",
    "        if \"log_resources\" in globals():\n",
    "            log_resources(\"predict_raw done\", force=True)\n",
    "        return pd.Series(final_preds, index=feats.index, name=TARGET_COL)\n",
    "    def ensure_calibration(self, features: FeatureBundle) -> None:\n",
    "        \"\"\"执行预测校准\"\"\"\n",
    "        # 定义校准参数的搜索边界 (scale, shift)\n",
    "        bounds = Bounds([0.8, -0.5], [1.2, 0.5])\n",
    "        # 在（部分）训练集上生成预测，用于计算校准参数\n",
    "        train_preds = self.predict_raw(features.train_features)\n",
    "        # 调用校准函数\n",
    "        calib = calibrate_predictions(\n",
    "            features.calibration_frame,\n",
    "            train_preds,\n",
    "            window=self.calibration_window,\n",
    "            bounds=bounds,\n",
    "            min_position=self.min_position,\n",
    "            max_position=self.max_position,\n",
    "        )\n",
    "        self.calibration = calib\n",
    "        # 将模型和校准的元数据保存为 JSON 文件，方便追溯和分析\n",
    "        payload = {\n",
    "            \"source\": self.model_source,\n",
    "            \"feature_columns\": self.feature_columns,\n",
    "            \"calibration\": {\n",
    "                \"scale\": calib.scale,\n",
    "                \"shift\": calib.shift,\n",
    "                \"raw_sharpe\": calib.raw_sharpe,\n",
    "                \"adjusted_sharpe\": calib.adjusted_sharpe,\n",
    "                \"window\": self.calibration_window,\n",
    "            },\n",
    "        }\n",
    "        self.paths.model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        (self.paths.metadata_path).write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    def predict(self, df: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"生成最终的、经过校准的预测\"\"\"\n",
    "        raw = self.predict_raw(df)\n",
    "        if self.calibration is None:\n",
    "            # 如果没有进行校准，直接返回原始预测\n",
    "            return raw\n",
    "        # 应用校准参数\n",
    "        adjusted = apply_calibration(\n",
    "            raw.values,\n",
    "            self.calibration.params,\n",
    "            min_position=self.min_position,\n",
    "            max_position=self.max_position,\n",
    "        )\n",
    "        return pd.Series(adjusted, index=raw.index, name=raw.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf3fb14d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T12:22:16.137540Z",
     "iopub.status.busy": "2025-12-14T12:22:16.137119Z",
     "iopub.status.idle": "2025-12-14T12:22:16.146294Z",
     "shell.execute_reply": "2025-12-14T12:22:16.145743Z"
    },
    "papermill": {
     "duration": 0.013649,
     "end_time": "2025-12-14T12:22:16.147434",
     "exception": false,
     "start_time": "2025-12-14T12:22:16.133785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 5) 端到端执行：训练/加载 + 推理 + 提交\n",
    "\n",
    "# 定义一个函数来运行整个流程\n",
    "def run_pipeline(train_if_needed: bool = True, max_training_rows: Optional[int] = 20000):\n",
    "    \"\"\"Run the pipeline with an optional training-row cap for faster experiments.\"\"\"\n",
    "    # 1. 解析路径\n",
    "    paths = resolve_paths(verbose=True)\n",
    "    if \"log_resources\" in globals():\n",
    "        log_resources(\"pipeline: paths\", force=True)\n",
    "    # 2. 加载数据\n",
    "    bundle = load_data(paths)\n",
    "    # 3. 构建特征\n",
    "    features = build_features(bundle)\n",
    "    # 4. 初始化模型服务\n",
    "    service = TabPFNService(paths, max_training_rows=max_training_rows)\n",
    "    # 5. 加载或训练模型\n",
    "    loaded = service.load() # 尝试加载预训练模型\n",
    "    if not loaded:\n",
    "        if not train_if_needed:\n",
    "            raise RuntimeError(\"No pretrained TabPFN model found and training is disabled.\")\n",
    "        print(\"No saved model found -> training TabPFN ...\")\n",
    "        if \"log_resources\" in globals():\n",
    "            log_resources(\"pipeline: before fit\", force=True)\n",
    "        service.fit(features) # 如果加载失败，则训练新模型\n",
    "        if \"log_resources\" in globals():\n",
    "            log_resources(\"pipeline: after fit\", force=True)\n",
    "    else:\n",
    "        print(f\"Loaded pretrained model from {paths.model_dir}\")\n",
    "\n",
    "    # 6. 执行校准\n",
    "    service.ensure_calibration(features)\n",
    "    print(\"Calibration scale/shift:\", service.calibration.scale, service.calibration.shift)\n",
    "\n",
    "    if \"log_resources\" in globals():\n",
    "        log_resources(\"pipeline: after calibration\", force=True)\n",
    "\n",
    "    # 7. 在测试集上生成最终预测\n",
    "    if \"log_resources\" in globals():\n",
    "        log_resources(\"pipeline: before test predict\", force=True)\n",
    "    test_preds = service.predict(features.test_features)\n",
    "    \n",
    "    # 8. 创建提交文件\n",
    "    # 竞赛网关逻辑：优先用 batch_id；否则使用 test.csv 第一列作为 row_id（本赛通常是 date_id）\n",
    "    if \"batch_id\" in bundle.test.columns:\n",
    "        row_col = \"batch_id\"\n",
    "    else:\n",
    "        row_col = bundle.test.columns[0] if len(bundle.test.columns) else \"row_id\"\n",
    "    ids = bundle.test[row_col].to_numpy() if row_col in bundle.test.columns else np.arange(len(test_preds))\n",
    "    submission = pd.DataFrame({row_col: ids, \"prediction\": np.asarray(test_preds)})\n",
    "    \n",
    "    # 9. Save strictly as Parquet using PyArrow (Stable)\n",
    "    # Hull Tactical requires submission.parquet\n",
    "    try:\n",
    "        submission.to_parquet(paths.submission_parquet, engine=\"pyarrow\", index=False)\n",
    "        print(f\"Saved submission Parquet (pyarrow): {submission.shape} -> {paths.submission_parquet}\")\n",
    "        if \"log_resources\" in globals():\n",
    "            log_resources(\"pipeline: after parquet\", force=True)\n",
    "    except Exception as exc:\n",
    "        print(f\"FATAL: Parquet export failed: {exc}\")\n",
    "        submission.to_csv(paths.submission_csv, index=False)\n",
    "        raise\n",
    "\n",
    "    return submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a7aff07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T12:22:16.153831Z",
     "iopub.status.busy": "2025-12-14T12:22:16.153621Z",
     "iopub.status.idle": "2025-12-14T12:24:14.918231Z",
     "shell.execute_reply": "2025-12-14T12:24:14.917524Z"
    },
    "papermill": {
     "duration": 118.768808,
     "end_time": "2025-12-14T12:24:14.919521",
     "exception": false,
     "start_time": "2025-12-14T12:22:16.150713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Paths ===\n",
      "data_root         : /kaggle/input/hull-tactical-market-prediction\n",
      "train_csv         : /kaggle/input/hull-tactical-market-prediction/train.csv\n",
      "test_csv          : /kaggle/input/hull-tactical-market-prediction/test.csv\n",
      "out_dir           : /kaggle/working\n",
      "model_dir         : /kaggle/working/tabpfn_model\n",
      "submission_csv    : /kaggle/working/submission.csv\n",
      "submission_parquet: /kaggle/working/submission.parquet\n",
      "metadata_path     : /kaggle/working/tabpfn_model/metadata.json\n",
      "Train rows: 8980 | Test rows: 10 | Overlap dates: 68\n",
      "Feature columns: 98\n",
      "No saved model found -> training TabPFN ...\n",
      "Calibration scale/shift: 1.1998252107984386 -0.015470320717753984\n",
      "Saved submission CSV: (10, 2) -> /kaggle/working/submission.csv\n",
      "Saved submission Parquet: polars -> /kaggle/working/submission.parquet\n",
      "耗时：118.75 秒\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8980</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8981</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8982</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8983</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8984</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_id  prediction\n",
       "0     8980         0.0\n",
       "1     8981         0.0\n",
       "2     8982         0.0\n",
       "3     8983         0.0\n",
       "4     8984         0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "submission_df = run_pipeline(train_if_needed=True)\n",
    "elapsed = time.perf_counter() - t0\n",
    "print(f\"耗时：{elapsed:.2f} 秒\")\n",
    "submission_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14861981,
     "sourceId": 111543,
     "sourceType": "competition"
    },
    {
     "datasetId": 8765127,
     "sourceId": 13771919,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8768648,
     "sourceId": 13776606,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9009498,
     "sourceId": 14142447,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 140.412679,
   "end_time": "2025-12-14T12:24:16.542164",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-14T12:21:56.129485",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}