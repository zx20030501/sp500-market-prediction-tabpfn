{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Kaggle Submission – TabPFN 单体版\n",
        "\n",
        "该 Notebook 完全使用 TabPFN，不依赖项目内模块文件。每个代码单元封装为相对独立的“模块”，便于后续拆分成独立 py 文件。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "13a9eac5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 0) 环境设置与路径管理\n",
        "# 从 __future__ 导入 annotations，确保类型提示的向前兼容性\n",
        "from __future__ import annotations\n",
        "# 导入数据类，用于创建结构化的数据容器\n",
        "from dataclasses import dataclass\n",
        "# 导入 pathlib，用于面向对象的文件系统路径操作\n",
        "from pathlib import Path\n",
        "# 导入 typing 中的高级类型提示\n",
        "from typing import Optional, Dict, List, Sequence, Tuple\n",
        "# 导入 os 模块，用于与操作系统交互，如此处设置环境变量\n",
        "import os\n",
        "# 导入 json 模块，用于处理 JSON 数据\n",
        "import json\n",
        "# 导入 numpy，用于高效的数值计算\n",
        "import numpy as np\n",
        "# 导入 pandas，用于数据处理和分析\n",
        "import pandas as pd\n",
        "\n",
        "# --- 全局常量和环境配置 ---\n",
        "\n",
        "# 检测是否在 Kaggle 环境中运行，Kaggle 的工作目录是 /kaggle/working\n",
        "IS_KAGGLE = Path('/kaggle/working').exists()\n",
        "# 获取当前工作目录\n",
        "CWD = Path.cwd()\n",
        "# 解析输入数据的基础路径。在 Kaggle 环境中，路径通常是 ../input/\n",
        "BASE_INPUT = (CWD.parent / \"input\") if CWD.name == \"working\" else (Path(\"..\") / \"input\")\n",
        "# 定义 Hugging Face 模型缓存的路径，指向预先下载好的数据集\n",
        "HF_CACHE = BASE_INPUT / \"package-hf-cache-tabpfn\" / \"hf_cache\"\n",
        "# TabPFN 模型的本地检查点文件路径\n",
        "LOCAL_CKPT = HF_CACHE / \"tabpfn-v2-regressor.ckpt\"\n",
        "# 设置 Hugging Face 的环境变量，使其使用本地缓存\n",
        "os.environ.setdefault(\"HF_HOME\", str(HF_CACHE))\n",
        "# 强制 Hugging Face Hub 使用离线模式，避免在 Kaggle Notebook 中联网下载\n",
        "os.environ.setdefault(\"HF_HUB_OFFLINE\", \"1\")\n",
        "\n",
        "# 使用 dataclass 定义一个用于存储所有重要路径的容器\n",
        "@dataclass\n",
        "class Paths:\n",
        "    data_root: Path          # 数据集根目录\n",
        "    train_csv: Path          # 训练数据 CSV 文件路径\n",
        "    test_csv: Path           # 测试数据 CSV 文件路径\n",
        "    out_dir: Path            # 输出目录\n",
        "    model_dir: Path          # 模型保存目录\n",
        "    submission_csv: Path     # 提交文件 (CSV 格式) 路径\n",
        "    submission_parquet: Path # 提交文件 (Parquet 格式) 路径\n",
        "    metadata_path: Path      # 模型元数据文件路径\n",
        "\n",
        "# 定义一个函数来解析并构建所有需要的路径\n",
        "def resolve_paths(verbose: bool = True) -> Paths:\n",
        "    # 比赛数据的根目录\n",
        "    data_root = (BASE_INPUT / \"hull-tactical-market-prediction\").resolve()\n",
        "    # 输出目录设置为当前工作目录\n",
        "    out_dir = CWD.resolve()\n",
        "    # 模型目录设置在输出目录下\n",
        "    model_dir = out_dir / \"tabpfn_model\"\n",
        "    # 创建并填充 Paths 对象\n",
        "    paths = Paths(\n",
        "        data_root=data_root,\n",
        "        train_csv=data_root / \"train.csv\",\n",
        "        test_csv=data_root / \"test.csv\",\n",
        "        out_dir=out_dir,\n",
        "        model_dir=model_dir,\n",
        "        submission_csv=out_dir / \"submission.csv\",\n",
        "        submission_parquet=out_dir / \"submission.parquet\",\n",
        "        metadata_path=model_dir / \"metadata.json\",\n",
        "    )\n",
        "    # 如果 verbose 为 True，则打印所有解析出的路径，方便调试\n",
        "    if verbose:\n",
        "        print(\"=== Paths ===\")\n",
        "        for k, v in paths.__dict__.items():\n",
        "            print(f\"{k:18s}: {v}\")\n",
        "    return paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5b05757c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not Kaggle environment; skip online install block\n"
          ]
        }
      ],
      "source": [
        "# 若在 Kaggle 环境中运行，则按需使用离线轮子（Kaggle 自带 GPU 版 torch）\n",
        "if IS_KAGGLE:\n",
        "    import os\n",
        "    import subprocess\n",
        "    from pathlib import Path\n",
        "\n",
        "    wheel_ds = os.environ.get(\"TABPFN_WHEEL_DATASET\", \"kaggle-linux-wheels-tabpfn\")\n",
        "    wheel_dir = Path(f\"/kaggle/input/{wheel_ds}\")\n",
        "    if (wheel_dir / \"kaggle-linux-wheels-tabpfn\").exists():\n",
        "        wheel_dir = wheel_dir / \"kaggle-linux-wheels-tabpfn\"\n",
        "    print(\"Wheel dir:\", wheel_dir)\n",
        "    print(\"提示：本赛提交需要 /kaggle/working/submission.parquet；请不要在 Kaggle 里 pip 安装/升级 pyarrow。\")\n",
        "\n",
        "    def pip_install(*packages: str, no_deps: bool = False) -> None:\n",
        "        cmd = [\"pip\", \"install\", \"--no-index\", f\"--find-links={wheel_dir}\"]\n",
        "        if no_deps:\n",
        "            cmd.append(\"--no-deps\")\n",
        "        cmd += list(packages)\n",
        "        subprocess.check_call(cmd)\n",
        "\n",
        "    # 关键：用 --no-deps 避免 pip 去解析/改动 torch 与 nvidia-*（Kaggle 已自带 GPU torch）\n",
        "    pip_install(\"backoff==2.2.1\", \"distro==1.9.0\", no_deps=True)\n",
        "    pip_install(\"posthog==6.7.4\", no_deps=True)\n",
        "    pip_install(\"tabpfn-common-utils==0.2.10\", \"kditransform==1.2.0\", \"eval-type-backport==0.3.1\", no_deps=True)\n",
        "    pip_install(\"tabpfn==6.0.6\", no_deps=True)\n",
        "\n",
        "    os.environ.setdefault(\"HF_HOME\", str(wheel_dir))\n",
        "    os.environ.setdefault(\"HF_HUB_OFFLINE\", \"1\")\n",
        "    os.environ.setdefault(\"TABPFN_MODEL_PATH\", str(wheel_dir / \"tabpfn-v2-regressor.ckpt\"))\n",
        "else:\n",
        "    print(\"Not Kaggle environment; skip online install block\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d384e719",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) 数据加载与预处理\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# 定义一个数据容器，用于捆绑加载后的训练集、测试集和重叠日期信息\n",
        "@dataclass\n",
        "class DataBundle:\n",
        "    train: pd.DataFrame\n",
        "    test: pd.DataFrame\n",
        "    overlap_dates: List[int]\n",
        "\n",
        "# 定义加载数据的函数\n",
        "def load_data(paths: Paths) -> DataBundle:\n",
        "    # 使用 pandas 从 CSV 文件加载训练和测试数据\n",
        "    train_df = pd.read_csv(paths.train_csv)\n",
        "    test_df = pd.read_csv(paths.test_csv)\n",
        "\n",
        "    # --- 处理时间序列泄漏 ---\n",
        "    # 竞赛说明中提到，训练集包含一部分与测试集时间重叠的数据（数据泄漏）。\n",
        "    # 为了建立一个稳健的模型，我们需要识别并移除这部分数据。\n",
        "\n",
        "    # 获取测试集的最早日期 ID\n",
        "    test_start = int(test_df[\"date_id\"].min())\n",
        "    # 创建一个布尔掩码，标记出训练集中日期大于等于测试集起始日期的行\n",
        "    leak_mask = train_df[\"date_id\"] >= test_start\n",
        "    # 提取这些重叠的、唯一的日期 ID\n",
        "    overlap_dates = sorted(train_df.loc[leak_mask, \"date_id\"].unique().tolist())\n",
        "\n",
        "    # 从训练集中移除存在泄漏的行\n",
        "    cleaned_train = (\n",
        "        train_df.loc[~leak_mask]\n",
        "        .sort_values(\"date_id\") # 按日期排序\n",
        "        .reset_index(drop=True) # 重置索引\n",
        "    )\n",
        "    # 对测试集也进行排序和索引重置，以确保数据一致性\n",
        "    cleaned_test = test_df.sort_values(\"date_id\").reset_index(drop=True)\n",
        "\n",
        "    # 打印清洗后数据的信息\n",
        "    print(f\"Train rows: {len(cleaned_train)} | Test rows: {len(cleaned_test)} | Overlap dates: {len(overlap_dates)}\")\n",
        "    # 返回包含清洗后数据的 DataBundle 对象\n",
        "    return DataBundle(train=cleaned_train, test=cleaned_test, overlap_dates=overlap_dates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8b207f14",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) 特征工程\n",
        "\n",
        "# --- 常量定义 ---\n",
        "# 目标变量列名\n",
        "TARGET_COL = \"forward_returns\"\n",
        "\n",
        "# 用于创建滞后特征的源列。这些是时间序列相关的关键指标。\n",
        "LAG_SOURCE_COLUMNS: Tuple[str, ...] = (\n",
        "    \"forward_returns\",                 # 未来收益率\n",
        "    \"market_forward_excess_returns\",   # 市场未来超额收益率\n",
        "    \"risk_free_rate\",                  # 无风险利率\n",
        ")\n",
        "\n",
        "# 用于识别\"前瞻性\"列的关键词。这些列包含未来信息，在训练时不能作为特征使用，否则会导致数据泄漏。\n",
        "FORWARD_KEYWORDS: Tuple[str, ...] = (\"forward\", \"future\", \"lead\", \"next\")\n",
        "# 安全的前缀。如果列名以这些前缀开头（例如 'lagged_forward_returns'），则不认为它是前瞻性的。\n",
        "SAFE_FORWARD_PREFIXES: Tuple[str, ...] = (\"lagged_\",)\n",
        "# 需要从特征集中明确排除的列。这些列要么是目标变量，要么是目标的变体，或不应作为特征。\n",
        "EXCLUDE_COLUMNS: Tuple[str, ...] = (\n",
        "    \"forward_returns\",\n",
        "    \"risk_free_rate\",\n",
        "    \"market_forward_excess_returns\",\n",
        ")\n",
        "\n",
        "# 辅助函数，判断一个列名是否表示前瞻性信息\n",
        "def _is_forward_looking(name: str) -> bool:\n",
        "    lower = name.lower()\n",
        "    # 如果列名以安全前缀开头，则不是前瞻性的\n",
        "    if any(lower.startswith(prefix) for prefix in SAFE_FORWARD_PREFIXES):\n",
        "        return False\n",
        "    # 如果列名包含任何前瞻性关键词，则认为是前瞻性的\n",
        "    return any(keyword in lower for keyword in FORWARD_KEYWORDS)\n",
        "\n",
        "# 定义一个数据容器，用于捆绑所有与特征相关的 DataFrame\n",
        "@dataclass\n",
        "class FeatureBundle:\n",
        "    feature_columns: List[str]      # 特征列名列表\n",
        "    train_features: pd.DataFrame   # 训练特征集 (X_train)\n",
        "    test_features: pd.DataFrame    # 测试特征集 (X_test)\n",
        "    train_with_target: pd.DataFrame # 带有目标列的训练集 (用于 TabPFN)\n",
        "    calibration_frame: pd.DataFrame # 用于后续 Sharpe 校准的数据\n",
        "\n",
        "# 主函数，构建所有特征\n",
        "def build_features(bundle: DataBundle) -> FeatureBundle:\n",
        "    train_df = bundle.train.copy()\n",
        "    test_df = bundle.test.copy()\n",
        "\n",
        "    # --- 创建滞后特征 ---\n",
        "    # 移位操作 (shift) 会将前一天的数据作为当前行的新特征，这是处理时间序列数据的常用方法。\n",
        "    for col in LAG_SOURCE_COLUMNS:\n",
        "        train_df[f\"lagged_{col}\"] = train_df[col].shift(1)\n",
        "\n",
        "    # 由于 shift(1) 会在第一行产生 NaN，需要删除这些行\n",
        "    lag_columns = [f\"lagged_{c}\" for c in LAG_SOURCE_COLUMNS]\n",
        "    train_df = train_df.dropna(subset=lag_columns).reset_index(drop=True)\n",
        "\n",
        "    # --- 筛选特征列 ---\n",
        "    base_exclude = set(EXCLUDE_COLUMNS)\n",
        "    feature_columns = [\n",
        "        col\n",
        "        for col in train_df.columns\n",
        "        # 确保特征在测试集中也存在\n",
        "        if col in test_df.columns\n",
        "        # 排除预定义的列\n",
        "        and col not in base_exclude\n",
        "        # 排除所有前瞻性的列\n",
        "        and not _is_forward_looking(col)\n",
        "    ]\n",
        "    feature_columns.sort() # 对列名排序，确保每次运行的顺序一致\n",
        "\n",
        "    # --- 准备最终的 DataFrame ---\n",
        "    # 使用 reindex 确保训练集和测试集有完全相同的列序，并用 0 填充缺失值\n",
        "    train_features = train_df.reindex(columns=feature_columns, fill_value=np.nan).fillna(0)\n",
        "    test_features = test_df.reindex(columns=feature_columns, fill_value=np.nan).fillna(0)\n",
        "    # 创建一个包含特征和目标列的 DataFrame，这是 TabPFN.fit() 方法需要的格式\n",
        "    train_with_target = train_features.copy()\n",
        "    train_with_target[TARGET_COL] = train_df[TARGET_COL].to_numpy(dtype=np.float64)\n",
        "    # 创建一个用于校准的 DataFrame，只包含必要列\n",
        "    calibration_frame = train_df[[\"date_id\", \"forward_returns\", \"risk_free_rate\"]].copy()\n",
        "\n",
        "    print(f\"Feature columns: {len(feature_columns)}\")\n",
        "    return FeatureBundle(\n",
        "        feature_columns=feature_columns,\n",
        "        train_features=train_features,\n",
        "        test_features=test_features,\n",
        "        train_with_target=train_with_target,\n",
        "        calibration_frame=calibration_frame,\n",
        "    )\n",
        "\n",
        "# 辅助函数，用于在预测时准备特征。确保应用与训练时相同的列和填充逻辑。\n",
        "def prepare_features(df: pd.DataFrame, feature_columns: Sequence[str]) -> pd.DataFrame:\n",
        "    return df.reindex(columns=feature_columns, fill_value=np.nan).fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f9ffe43b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Sharpe Ratio 校准工具\n",
        "# 本模块的目标是找到一个线性变换 (y = scale * x + shift) 应用于模型的原始预测，\n",
        "# 使得变换后的头寸（position）在历史数据上能最大化一个自定义的“调整后夏普比率”。\n",
        "# 这是一种后处理步骤，旨在使模型的输出更符合竞赛的特定评分标准。\n",
        "from dataclasses import dataclass\n",
        "from scipy.optimize import minimize, Bounds # 从 SciPy 导入优化函数和边界类\n",
        "from warnings import warn\n",
        "\n",
        "# 定义一个数据容器，用于存储校准结果\n",
        "@dataclass\n",
        "class CalibrationResult:\n",
        "    scale: float             # 缩放因子\n",
        "    shift: float             # 平移因子\n",
        "    raw_sharpe: float        # 原始预测的夏普比率\n",
        "    adjusted_sharpe: float   # 校准后预测的夏普比率\n",
        "\n",
        "    @property\n",
        "    def params(self):\n",
        "        \"\"\"返回一个包含 scale 和 shift 的元组\"\"\"\n",
        "        return (self.scale, self.shift)\n",
        "\n",
        "# 计算调整后夏普比率的函数\n",
        "def adjusted_sharpe(solution: pd.DataFrame, positions: np.ndarray, min_position: float, max_position: float) -> float:\n",
        "    if solution.empty:\n",
        "        return 0.0\n",
        "    df = solution[[\"forward_returns\", \"risk_free_rate\"]].copy()\n",
        "    # 将头寸限制在允许的最小和最大值之间\n",
        "    clipped_positions = np.clip(np.asarray(positions, dtype=np.float64), min_position, max_position)\n",
        "    df[\"position\"] = clipped_positions\n",
        "    # 计算策略的日收益率：一部分是无风险资产，一部分是根据头寸投资于风险资产\n",
        "    strategy_returns = df[\"risk_free_rate\"] * (1 - df[\"position\"]) + df[\"position\"] * df[\"forward_returns\"]\n",
        "    # 计算策略的超额收益率\n",
        "    excess = strategy_returns - df[\"risk_free_rate\"]\n",
        "    if len(excess) == 0: return 0.0\n",
        "    # 计算几何平均超额收益率\n",
        "    cum_excess = float(np.prod(1 + excess))\n",
        "    mean_excess = cum_excess ** (1 / len(df)) - 1\n",
        "    # 计算策略收益率的标准差\n",
        "    std_excess = float(strategy_returns.std(ddof=0))\n",
        "    if std_excess == 0: return 0.0\n",
        "    annual_days = 252 # 年化天数\n",
        "    # 计算标准夏普比率（年化）\n",
        "    sharpe = mean_excess / std_excess * np.sqrt(annual_days)\n",
        "\n",
        "    # --- 计算惩罚项 ---\n",
        "    # 市场超额收益\n",
        "    market_excess = df[\"forward_returns\"] - df[\"risk_free_rate\"]\n",
        "    market_cum = float(np.prod(1 + market_excess))\n",
        "    market_mean = market_cum ** (1 / len(df)) - 1 if len(df) else 0.0\n",
        "    market_std = float(market_excess.std(ddof=0))\n",
        "    # 年化波动率\n",
        "    market_vol = market_std * np.sqrt(annual_days) * 100\n",
        "    strat_vol = std_excess * np.sqrt(annual_days) * 100\n",
        "    # 1. 波动率惩罚：如果策略波动率超过市场波动率的1.2倍，则施加惩罚\n",
        "    excess_vol_penalty = 1 + max(0, strat_vol / market_vol - 1.2) if market_vol > 0 else 1\n",
        "    # 2. 收益差距惩罚：如果策略的平均收益低于市场平均收益，则施加惩罚\n",
        "    return_gap = max(0, (market_mean - mean_excess) * 100 * annual_days)\n",
        "    return_penalty = 1 + (return_gap ** 2) / 100\n",
        "    # 最终分数为夏普比率除以两个惩罚项\n",
        "    score = sharpe / (excess_vol_penalty * return_penalty)\n",
        "    return float(min(score, 1_000_000)) # 返回分数，并设置一个上限\n",
        "\n",
        "# 应用校准参数（scale 和 shift）到预测值上\n",
        "def apply_calibration(values: np.ndarray, params: Tuple[float, float], min_position: float, max_position: float) -> np.ndarray:\n",
        "    scale, shift = params\n",
        "    arr = np.asarray(values, dtype=np.float64)\n",
        "    # 应用线性变换并裁剪结果\n",
        "    return np.clip(arr * scale + shift, min_position, max_position)\n",
        "\n",
        "# 对预测进行校准的主函数\n",
        "def calibrate_predictions(train_frame: pd.DataFrame, base_predictions: pd.Series, window: int, bounds: Bounds, min_position: float, max_position: float) -> CalibrationResult:\n",
        "    if base_predictions is None or len(base_predictions) == 0:\n",
        "        return CalibrationResult(scale=1.0, shift=0.0, raw_sharpe=0.0, adjusted_sharpe=0.0)\n",
        "    work = train_frame[[\"date_id\", \"forward_returns\", \"risk_free_rate\"]].copy()\n",
        "    work[\"model_pred\"] = base_predictions.values\n",
        "    work = work.sort_values(\"date_id\")\n",
        "    # 使用一个滑动窗口（例如最近180天）的数据进行校准\n",
        "    if len(work) > window:\n",
        "        work = work.tail(window)\n",
        "    work = work.set_index(\"date_id\")\n",
        "\n",
        "    # 定义优化目标函数。优化器会尝试找到使这个函数值最小的参数。\n",
        "    # 我们返回负的夏普比率，因为 minimize 求的是最小值。\n",
        "    def objective(params: np.ndarray) -> float:\n",
        "        adjusted = apply_calibration(work[\"model_pred\"].values, tuple(params), min_position, max_position)\n",
        "        return -adjusted_sharpe(work[[\"forward_returns\", \"risk_free_rate\"]], adjusted, min_position, max_position)\n",
        "\n",
        "    initial = np.array([1.0, 0.0]) # 初始猜测值 (scale=1, shift=0)\n",
        "    try:\n",
        "        # 调用 scipy.optimize.minimize 进行优化\n",
        "        result = minimize(\n",
        "            objective,             # 目标函数\n",
        "            x0=initial,            # 初始参数\n",
        "            method=\"Powell\",       # 优化算法\n",
        "            bounds=bounds,         # 参数的边界\n",
        "            options={\"xtol\": 1e-4, \"ftol\": 1e-4, \"maxiter\": 500}, # 优化选项\n",
        "        )\n",
        "        if not result.success:\n",
        "            warn(f\"Calibration did not converge: {result.message}\")\n",
        "            scale, shift = initial\n",
        "        else:\n",
        "            scale, shift = result.x # 获取优化后的参数\n",
        "    except Exception as exc:\n",
        "        warn(f\"Calibration failed, using identity transform: {exc}\")\n",
        "        scale, shift = initial\n",
        "\n",
        "    # 计算校准前后的夏普比率以供参考\n",
        "    adjusted = apply_calibration(work[\"model_pred\"].values, (scale, shift), min_position, max_position)\n",
        "    raw_score = adjusted_sharpe(work[[\"forward_returns\", \"risk_free_rate\"]], work[\"model_pred\"].values, min_position, max_position)\n",
        "    adj_score = adjusted_sharpe(work[[\"forward_returns\", \"risk_free_rate\"]], adjusted, min_position, max_position)\n",
        "    return CalibrationResult(scale=float(scale), shift=float(shift), raw_sharpe=raw_score, adjusted_sharpe=adj_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c5204c9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) TabPFN 模型服务封装\n",
        "# 这个类将 TabPFN 模型的所有操作（训练、加载、预测、校准）封装起来，提供一个简洁的接口。\n",
        "from tabpfn import TabPFNRegressor\n",
        "import joblib # 用于保存和加载 Python 对象（如此处的模型）\n",
        "import torch\n",
        "import os\n",
        "from scipy.optimize import Bounds\n",
        "import numpy as np\n",
        "\n",
        "class TabPFNService:\n",
        "    def __init__(self, paths: Paths, seed: int = 42, min_position: float = 0.0, max_position: float = 2.0, calibration_window: int = 180, max_training_rows: Optional[int] = 20000):\n",
        "        self.paths = paths\n",
        "        self.seed = seed\n",
        "        self.min_position = min_position\n",
        "        self.max_position = max_position\n",
        "        self.calibration_window = calibration_window\n",
        "        self.max_training_rows = max_training_rows  # limit training samples for runtime control\n",
        "        self.model: Optional[TabPFNRegressor] = None # 模型实例\n",
        "        self.feature_columns: List[str] = [] # 特征列名\n",
        "        self.calibration: Optional[CalibrationResult] = None # 校准结果\n",
        "        self.model_source: str = \"unknown\" # 记录模型来源（'trained' 或 'pretrained'）\n",
        "\n",
        "    @property\n",
        "    def device(self) -> str:\n",
        "        \"\"\"自动检测并返回可用的设备（CUDA GPU 或 CPU）\"\"\"\n",
        "        return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    def fit(self, features: FeatureBundle) -> None:\n",
        "        \"\"\"训练 TabPFN 模型并保存到文件\"\"\"\n",
        "        self.paths.model_dir.mkdir(parents=True, exist_ok=True)\n",
        "        # 初始化 TabPFN 回归器\n",
        "        # model_path: 指向本地的 .ckpt 文件\n",
        "        # n_estimators: 类似于集成方法中的树的数量，TabPFN 用它来做多次推理并平均\n",
        "        # ignore_pretraining_limits: TabPFN 预训练时对特征数量（100）和样本数量（1024）有限制，设置此项为 True 可忽略这些限制\n",
        "        model = TabPFNRegressor(model_path=os.environ.get(\"TABPFN_MODEL_PATH\", str(LOCAL_CKPT)), n_estimators=2, device=self.device, random_state=self.seed, ignore_pretraining_limits=True)\n",
        "        # 训练模型\n",
        "        train_X = features.train_features\n",
        "        train_y = features.train_with_target[TARGET_COL]\n",
        "        original_rows = len(train_X)\n",
        "        if self.max_training_rows is not None and original_rows > self.max_training_rows:\n",
        "            rng = np.random.default_rng(self.seed)\n",
        "            idx = rng.choice(original_rows, self.max_training_rows, replace=False)\n",
        "            train_X = train_X.iloc[idx].reset_index(drop=True)\n",
        "            train_y = train_y.iloc[idx].reset_index(drop=True)\n",
        "            print(f\"TabPFN fit: downsampled from {original_rows} -> {len(train_X)} rows for faster training\")\n",
        "        model.fit(train_X, train_y)\n",
        "        # 将训练好的模型和特征列名打包保存\n",
        "        payload = {\"model\": model, \"feature_columns\": features.feature_columns}\n",
        "        joblib.dump(payload, self.paths.model_dir / \"tabpfn_model.joblib\")\n",
        "        # 更新服务实例的状态\n",
        "        self.model = model\n",
        "        self.feature_columns = list(features.feature_columns)\n",
        "        self.model_source = \"trained\"\n",
        "\n",
        "    def load(self) -> bool:\n",
        "        \"\"\"从文件加载预训练的模型\"\"\"\n",
        "        model_path = self.paths.model_dir / \"tabpfn_model.joblib\"\n",
        "        if not model_path.exists():\n",
        "            return False # 如果模型文件不存在，返回 False\n",
        "        saved = joblib.load(model_path)\n",
        "        self.model = saved[\"model\"]\n",
        "        self.feature_columns = list(saved[\"feature_columns\"])\n",
        "        self.model_source = \"pretrained\"\n",
        "        return True\n",
        "\n",
        "    def predict_raw(self, df: pd.DataFrame) -> pd.Series:\n",
        "        \"\"\"使用模型生成原始预测（校准前）\"\"\"\n",
        "        # 确保输入数据的特征与训练时一致\n",
        "        feats = prepare_features(df, self.feature_columns)\n",
        "        preds = self.model.predict(feats)\n",
        "        return pd.Series(preds, index=feats.index, name=TARGET_COL)\n",
        "\n",
        "    def ensure_calibration(self, features: FeatureBundle) -> None:\n",
        "        \"\"\"执行预测校准\"\"\"\n",
        "        # 定义校准参数的搜索边界 (scale, shift)\n",
        "        bounds = Bounds([0.8, -0.5], [1.2, 0.5])\n",
        "        # 在（部分）训练集上生成预测，用于计算校准参数\n",
        "        train_preds = self.predict_raw(features.train_features)\n",
        "        # 调用校准函数\n",
        "        calib = calibrate_predictions(\n",
        "            features.calibration_frame,\n",
        "            train_preds,\n",
        "            window=self.calibration_window,\n",
        "            bounds=bounds,\n",
        "            min_position=self.min_position,\n",
        "            max_position=self.max_position,\n",
        "        )\n",
        "        self.calibration = calib\n",
        "        # 将模型和校准的元数据保存为 JSON 文件，方便追溯和分析\n",
        "        payload = {\n",
        "            \"source\": self.model_source,\n",
        "            \"feature_columns\": self.feature_columns,\n",
        "            \"calibration\": {\n",
        "                \"scale\": calib.scale,\n",
        "                \"shift\": calib.shift,\n",
        "                \"raw_sharpe\": calib.raw_sharpe,\n",
        "                \"adjusted_sharpe\": calib.adjusted_sharpe,\n",
        "                \"window\": self.calibration_window,\n",
        "            },\n",
        "        }\n",
        "        self.paths.model_dir.mkdir(parents=True, exist_ok=True)\n",
        "        (self.paths.metadata_path).write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "    def predict(self, df: pd.DataFrame) -> pd.Series:\n",
        "        \"\"\"生成最终的、经过校准的预测\"\"\"\n",
        "        raw = self.predict_raw(df)\n",
        "        if self.calibration is None:\n",
        "            # 如果没有进行校准，直接返回原始预测\n",
        "            return raw\n",
        "        # 应用校准参数\n",
        "        adjusted = apply_calibration(\n",
        "            raw.values,\n",
        "            self.calibration.params,\n",
        "            min_position=self.min_position,\n",
        "            max_position=self.max_position,\n",
        "        )\n",
        "        return pd.Series(adjusted, index=raw.index, name=raw.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "cc4f34ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5) 端到端执行：训练/加载 + 推理 + 提交\n",
        "\n",
        "# 定义一个函数来运行整个流程\n",
        "def run_pipeline(train_if_needed: bool = True, max_training_rows: Optional[int] = 20000):\n",
        "    \"\"\"Run the pipeline with an optional training-row cap for faster experiments.\"\"\"\n",
        "    # 1. 解析路径\n",
        "    paths = resolve_paths(verbose=True)\n",
        "    # 2. 加载数据\n",
        "    bundle = load_data(paths)\n",
        "    # 3. 构建特征\n",
        "    features = build_features(bundle)\n",
        "    # 4. 初始化模型服务\n",
        "    service = TabPFNService(paths, max_training_rows=max_training_rows)\n",
        "    # 5. 加载或训练模型\n",
        "    loaded = service.load() # 尝试加载预训练模型\n",
        "    if not loaded:\n",
        "        if not train_if_needed:\n",
        "            raise RuntimeError(\"No pretrained TabPFN model found and training is disabled.\")\n",
        "        print(\"No saved model found -> training TabPFN ...\")\n",
        "        service.fit(features) # 如果加载失败，则训练新模型\n",
        "    else:\n",
        "        print(f\"Loaded pretrained model from {paths.model_dir}\")\n",
        "\n",
        "    # 6. 执行校准\n",
        "    service.ensure_calibration(features)\n",
        "    print(\"Calibration scale/shift:\", service.calibration.scale, service.calibration.shift)\n",
        "\n",
        "    # 7. 在测试集上生成最终预测\n",
        "    test_preds = service.predict(features.test_features)\n",
        "    \n",
        "    # 8. 创建提交文件\n",
        "    # 竞赛网关逻辑：优先用 batch_id；否则使用 test.csv 第一列作为 row_id（本赛通常是 date_id）\n",
        "    if \"batch_id\" in bundle.test.columns:\n",
        "        row_col = \"batch_id\"\n",
        "    else:\n",
        "        row_col = bundle.test.columns[0] if len(bundle.test.columns) else \"row_id\"\n",
        "    ids = bundle.test[row_col].to_numpy() if row_col in bundle.test.columns else np.arange(len(test_preds))\n",
        "    submission = pd.DataFrame({row_col: ids, \"prediction\": np.asarray(test_preds)})\n",
        "    \n",
        "    # 9. 保存提交文件（Kaggle 代码赛需要 submission.parquet；CSV 仅用于排查/本地调试）\n",
        "    submission.to_csv(paths.submission_csv, index=False)\n",
        "    print(\"Saved submission CSV:\", submission.shape, \"->\", paths.submission_csv)\n",
        "    \n",
        "    try:\n",
        "        if IS_KAGGLE:\n",
        "            # 优先用 polars 写 parquet（不依赖 pyarrow，避免版本/二进制问题）\n",
        "            try:\n",
        "                import polars as pl\n",
        "                pl_df = pl.DataFrame({c: submission[c].to_numpy() for c in submission.columns})\n",
        "                pl_df.write_parquet(paths.submission_parquet, statistics=False)\n",
        "                parquet_engine = \"polars\"\n",
        "            except Exception:\n",
        "                submission.to_parquet(paths.submission_parquet, index=False)\n",
        "                parquet_engine = \"pandas/pyarrow\"\n",
        "        else:\n",
        "            submission.to_parquet(paths.submission_parquet, index=False)\n",
        "            parquet_engine = \"pandas/pyarrow\"\n",
        "        print(\"Saved submission Parquet:\", parquet_engine, \"->\", paths.submission_parquet)\n",
        "    except Exception as exc:\n",
        "        print(\"Parquet export failed:\", type(exc).__name__, str(exc))\n",
        "        raise\n",
        "    return submission\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1078ebde",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Paths ===\n",
            "data_root         : D:\\OneDrive\\Project\\kaggle-hull\\hull-tabpfn\\hull-tabpfn-r7000\\kaggle\\input\\hull-tactical-market-prediction\n",
            "train_csv         : D:\\OneDrive\\Project\\kaggle-hull\\hull-tabpfn\\hull-tabpfn-r7000\\kaggle\\input\\hull-tactical-market-prediction\\train.csv\n",
            "test_csv          : D:\\OneDrive\\Project\\kaggle-hull\\hull-tabpfn\\hull-tabpfn-r7000\\kaggle\\input\\hull-tactical-market-prediction\\test.csv\n",
            "out_dir           : D:\\OneDrive\\Project\\kaggle-hull\\hull-tabpfn\\hull-tabpfn-r7000\\kaggle\\working\n",
            "model_dir         : D:\\OneDrive\\Project\\kaggle-hull\\hull-tabpfn\\hull-tabpfn-r7000\\kaggle\\working\\tabpfn_model\n",
            "submission_csv    : D:\\OneDrive\\Project\\kaggle-hull\\hull-tabpfn\\hull-tabpfn-r7000\\kaggle\\working\\submission.csv\n",
            "submission_parquet: D:\\OneDrive\\Project\\kaggle-hull\\hull-tabpfn\\hull-tabpfn-r7000\\kaggle\\working\\submission.parquet\n",
            "metadata_path     : D:\\OneDrive\\Project\\kaggle-hull\\hull-tabpfn\\hull-tabpfn-r7000\\kaggle\\working\\tabpfn_model\\metadata.json\n",
            "Train rows: 8980 | Test rows: 10 | Overlap dates: 10\n",
            "Feature columns: 98\n",
            "Loaded pretrained model from D:\\OneDrive\\Project\\kaggle-hull\\hull-tabpfn\\hull-tabpfn-r7000\\kaggle\\working\\tabpfn_model\n",
            "Calibration scale/shift: 1.0355602049564945 -0.002851502753353999\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m      3\u001b[39m t0 = time.perf_counter()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m submission_df = \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_if_needed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m elapsed = time.perf_counter() - t0\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m耗时：\u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m 秒\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mrun_pipeline\u001b[39m\u001b[34m(train_if_needed)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCalibration scale/shift:\u001b[39m\u001b[33m\"\u001b[39m, service.calibration.scale, service.calibration.shift)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# 7. 在测试集上生成最终预测\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m test_preds = \u001b[43mservice\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtest_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# 8. 创建提交文件\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# 根据测试集中的 ID 列名 ('row_id' 或 'id') 创建提价文件\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mrow_id\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m bundle.test.columns:\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mTabPFNService.predict\u001b[39m\u001b[34m(self, df)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, df: pd.DataFrame) -> pd.Series:\n\u001b[32m     93\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"生成最终的、经过校准的预测\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     raw = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.calibration \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     96\u001b[39m         \u001b[38;5;66;03m# 如果没有进行校准，直接返回原始预测\u001b[39;00m\n\u001b[32m     97\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m raw\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mTabPFNService.predict_raw\u001b[39m\u001b[34m(self, df)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# 确保输入数据的特征与训练时一致\u001b[39;00m\n\u001b[32m     57\u001b[39m feats = prepare_features(df, \u001b[38;5;28mself\u001b[39m.feature_columns)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m preds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pd.Series(preds, index=feats.index, name=TARGET_COL)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramData\\PythonEnv\\hull-tabpfn\\Lib\\contextlib.py:81\u001b[39m, in \u001b[36mContextDecorator.__call__.<locals>.inner\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args, **kwds):\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._recreate_cm():\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramData\\PythonEnv\\hull-tabpfn\\Lib\\site-packages\\tabpfn_common_utils\\telemetry\\core\\decorators.py:288\u001b[39m, in \u001b[36mtrack_model_call.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: Any, **kwargs: Any) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_safe_call_with_telemetry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_names\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramData\\PythonEnv\\hull-tabpfn\\Lib\\site-packages\\tabpfn_common_utils\\telemetry\\core\\decorators.py:332\u001b[39m, in \u001b[36m_safe_call_with_telemetry\u001b[39m\u001b[34m(func, args, kwargs, model_method, param_names)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# Step 2: Run the actual function\u001b[39;00m\n\u001b[32m    331\u001b[39m start = time.perf_counter()\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m duration_ms = \u001b[38;5;28mint\u001b[39m((time.perf_counter() - start) * \u001b[32m1000\u001b[39m)\n\u001b[32m    335\u001b[39m \u001b[38;5;66;03m# Step 3: Send telemetry event\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramData\\PythonEnv\\hull-tabpfn\\Lib\\site-packages\\tabpfn\\regressor.py:960\u001b[39m, in \u001b[36mTabPFNRegressor.predict\u001b[39m\u001b[34m(self, X, output_type, quantiles)\u001b[39m\n\u001b[32m    953\u001b[39m X = process_text_na_dataframe(X, ord_encoder=\u001b[38;5;28mself\u001b[39m.preprocessor_)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    955\u001b[39m \u001b[38;5;66;03m# Runs over iteration engine\u001b[39;00m\n\u001b[32m    956\u001b[39m (\n\u001b[32m    957\u001b[39m     _,\n\u001b[32m    958\u001b[39m     outputs,  \u001b[38;5;66;03m# list of tensors [N_est, N_samples, N_borders] (after forward)\u001b[39;00m\n\u001b[32m    959\u001b[39m     borders,  \u001b[38;5;66;03m# list of numpy arrays containing borders for each estimator\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_inference_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[38;5;66;03m# --- Translate probs, average, get final logits ---\u001b[39;00m\n\u001b[32m    963\u001b[39m transformed_logits = [\n\u001b[32m    964\u001b[39m     translate_probs_across_borders(\n\u001b[32m    965\u001b[39m         logits,\n\u001b[32m   (...)\u001b[39m\u001b[32m    969\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m logits, borders_t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(outputs, borders)\n\u001b[32m    970\u001b[39m ]\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramData\\PythonEnv\\hull-tabpfn\\Lib\\site-packages\\tabpfn\\regressor.py:1088\u001b[39m, in \u001b[36mTabPFNRegressor.forward\u001b[39m\u001b[34m(self, X, use_inference_mode)\u001b[39m\n\u001b[32m   1085\u001b[39m borders: \u001b[38;5;28mlist\u001b[39m[np.ndarray] = []\n\u001b[32m   1087\u001b[39m \u001b[38;5;66;03m# Iterate over estimators\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1088\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecutor_\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1089\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevices_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautocast\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43muse_autocast_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: PLW2901\u001b[39;49;00m\n\u001b[32m   1094\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msoftmax_temperature\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramData\\PythonEnv\\hull-tabpfn\\Lib\\site-packages\\tabpfn\\inference.py:547\u001b[39m, in \u001b[36mInferenceEngineCachePreprocessing.iter_outputs\u001b[39m\u001b[34m(self, X, devices, autocast, only_return_standard_out)\u001b[39m\n\u001b[32m    531\u001b[39m model_forward_functions = (\n\u001b[32m    532\u001b[39m     partial(\n\u001b[32m    533\u001b[39m         \u001b[38;5;28mself\u001b[39m._call_model,\n\u001b[32m   (...)\u001b[39m\u001b[32m    543\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m sorted_indices\n\u001b[32m    544\u001b[39m )\n\u001b[32m    545\u001b[39m outputs = parallel_execute(devices, model_forward_functions)\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorted_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_move_and_squeeze_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mensemble_configs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.inference_mode:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramData\\PythonEnv\\hull-tabpfn\\Lib\\site-packages\\tabpfn\\parallel_execute.py:61\u001b[39m, in \u001b[36mparallel_execute\u001b[39m\u001b[34m(devices, functions)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Evaluate the given functions in parallel across `devices`.\u001b[39;00m\n\u001b[32m     43\u001b[39m \n\u001b[32m     44\u001b[39m \u001b[33;03mThe function evaluations are parallelised using Python threads, so this will only\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     57\u001b[39m \u001b[33;03m    as `functions`.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(devices) == \u001b[32m1\u001b[39m:\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# If we only have one device then just use the current thread to avoid overhead.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _execute_in_current_thread(devices[\u001b[32m0\u001b[39m], functions)\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _execute_with_multithreading(devices, functions)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramData\\PythonEnv\\hull-tabpfn\\Lib\\site-packages\\tabpfn\\parallel_execute.py:70\u001b[39m, in \u001b[36m_execute_in_current_thread\u001b[39m\u001b[34m(device, functions)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_execute_in_current_thread\u001b[39m(\n\u001b[32m     67\u001b[39m     device: torch.device, functions: Iterable[ParallelFunction[R_co]]\n\u001b[32m     68\u001b[39m ) -> Generator[R_co]:\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m function \u001b[38;5;129;01min\u001b[39;00m functions:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_parallel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramData\\PythonEnv\\hull-tabpfn\\Lib\\site-packages\\tabpfn\\inference.py:592\u001b[39m, in \u001b[36mInferenceEngineCachePreprocessing._call_model\u001b[39m\u001b[34m(self, device, is_parallel, X_train, X_test, y_train, cat_ix, autocast, only_return_standard_out, model_index, save_peak_mem)\u001b[39m\n\u001b[32m    586\u001b[39m batched_cat_ix = [cat_ix]\n\u001b[32m    588\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m    589\u001b[39m     get_autocast_context(device, enabled=autocast),\n\u001b[32m    590\u001b[39m     torch.inference_mode(\u001b[38;5;28mself\u001b[39m.inference_mode),\n\u001b[32m    591\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m592\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_full\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m        \u001b[49m\u001b[43monly_return_standard_out\u001b[49m\u001b[43m=\u001b[49m\u001b[43monly_return_standard_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcategorical_inds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatched_cat_ix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramData\\PythonEnv\\hull-tabpfn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramData\\PythonEnv\\hull-tabpfn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramData\\PythonEnv\\hull-tabpfn\\Lib\\site-packages\\tabpfn\\architectures\\base\\transformer.py:563\u001b[39m, in \u001b[36mPerFeatureTransformer.forward\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    557\u001b[39m     embedded_input, single_eval_pos = \u001b[38;5;28mself\u001b[39m.add_thinking_tokens(\n\u001b[32m    558\u001b[39m         embedded_input,\n\u001b[32m    559\u001b[39m         single_eval_pos,\n\u001b[32m    560\u001b[39m     )\n\u001b[32m    562\u001b[39m recompute_layer = \u001b[38;5;28mself\u001b[39m.recompute_layer \u001b[38;5;129;01mor\u001b[39;00m force_recompute_layer\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m encoder_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m        \u001b[49m\u001b[43membedded_input\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer_decoder\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43membedded_input\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43msingle_eval_pos\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m    \u001b[49m\u001b[43msingle_eval_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43msingle_eval_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_trainset_representation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcache_trainset_representation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrecompute_layer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecompute_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# b s f+1 e -> b s f+1 e\u001b[39;00m\n\u001b[32m    574\u001b[39m \u001b[38;5;66;03m# If we are using a decoder\u001b[39;00m\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transformer_decoder:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramData\\PythonEnv\\hull-tabpfn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramData\\PythonEnv\\hull-tabpfn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramData\\PythonEnv\\hull-tabpfn\\Lib\\site-packages\\tabpfn\\architectures\\base\\transformer.py:87\u001b[39m, in \u001b[36mLayerStack.forward\u001b[39m\u001b[34m(self, x, recompute_layer, **kwargs)\u001b[39m\n\u001b[32m     85\u001b[39m         x = checkpoint(partial(layer, **kwargs), x, use_reentrant=\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m         x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramData\\PythonEnv\\hull-tabpfn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramData\\PythonEnv\\hull-tabpfn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramData\\PythonEnv\\hull-tabpfn\\Lib\\site-packages\\tabpfn\\architectures\\base\\layer.py:421\u001b[39m, in \u001b[36mPerFeatureEncoderLayer.forward\u001b[39m\u001b[34m(self, state, single_eval_pos, cache_trainset_representation, att_src)\u001b[39m\n\u001b[32m    411\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    412\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPre-norm implementation is wrong, as the residual should never\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    413\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m be layer normed here.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    414\u001b[39m     )\n\u001b[32m    415\u001b[39m     state = layer_norm(\n\u001b[32m    416\u001b[39m         state,\n\u001b[32m    417\u001b[39m         allow_inplace=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    418\u001b[39m         save_peak_mem_factor=save_peak_mem_factor,\n\u001b[32m    419\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m state = \u001b[43msublayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pre_norm:\n\u001b[32m    423\u001b[39m     state = layer_norm(\n\u001b[32m    424\u001b[39m         state,\n\u001b[32m    425\u001b[39m         allow_inplace=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    426\u001b[39m         save_peak_mem_factor=save_peak_mem_factor,\n\u001b[32m    427\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramData\\PythonEnv\\hull-tabpfn\\Lib\\site-packages\\tabpfn\\architectures\\base\\layer.py:319\u001b[39m, in \u001b[36mPerFeatureEncoderLayer.forward.<locals>.attn_between_items\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.multiquery_item_attention_for_test_set:\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m single_eval_pos < x.shape[\u001b[32m1\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m         new_x_test = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn_between_items\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m            \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msingle_eval_pos\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m            \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43msingle_eval_pos\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msingle_eval_pos\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m            \u001b[49m\u001b[43msave_peak_mem_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_peak_mem_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_kv\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m            \u001b[49m\u001b[43madd_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m            \u001b[49m\u001b[43mallow_inplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_cached_kv\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msingle_eval_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreuse_first_head_kv\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    332\u001b[39m         new_x_test = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramData\\PythonEnv\\hull-tabpfn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramData\\PythonEnv\\hull-tabpfn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramData\\PythonEnv\\hull-tabpfn\\Lib\\site-packages\\tabpfn\\architectures\\base\\attention\\full_attention.py:354\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x, x_kv, cache_kv, add_input, allow_inplace, save_peak_mem_factor, reuse_first_head_kv, only_cache_first_head_kv, use_cached_kv)\u001b[39m\n\u001b[32m    337\u001b[39m         \u001b[38;5;28mself\u001b[39m._k_cache = torch.empty(\n\u001b[32m    338\u001b[39m             batch_size,\n\u001b[32m    339\u001b[39m             seqlen_kv,\n\u001b[32m   (...)\u001b[39m\u001b[32m    343\u001b[39m             dtype=x.dtype,\n\u001b[32m    344\u001b[39m         )\n\u001b[32m    345\u001b[39m         \u001b[38;5;28mself\u001b[39m._v_cache = torch.empty(\n\u001b[32m    346\u001b[39m             batch_size,\n\u001b[32m    347\u001b[39m             seqlen_kv,\n\u001b[32m   (...)\u001b[39m\u001b[32m    351\u001b[39m             dtype=x.dtype,\n\u001b[32m    352\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m output: torch.Tensor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_k_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_v_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_kv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cached_kv\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cached_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_inplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_inplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_peak_mem_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_peak_mem_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreuse_first_head_kv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreuse_first_head_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output.reshape(x_shape_after_transpose[:-\u001b[32m1\u001b[39m] + output.shape[-\u001b[32m1\u001b[39m:])\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramData\\PythonEnv\\hull-tabpfn\\Lib\\site-packages\\tabpfn\\architectures\\base\\memory.py:100\u001b[39m, in \u001b[36msupport_save_peak_mem_factor.<locals>.method_\u001b[39m\u001b[34m(self, x, add_input, allow_inplace, save_peak_mem_factor, *args, **kwargs)\u001b[39m\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m add_input:\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x + \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, x, *args, **kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramData\\PythonEnv\\hull-tabpfn\\Lib\\site-packages\\tabpfn\\architectures\\base\\attention\\full_attention.py:494\u001b[39m, in \u001b[36mMultiHeadAttention._compute\u001b[39m\u001b[34m(self, x, x_kv, k_cache, v_cache, kv_cache, cache_kv, use_cached_kv, reuse_first_head_kv)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;129m@support_save_peak_mem_factor\u001b[39m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_compute\u001b[39m(\n\u001b[32m    480\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    489\u001b[39m     reuse_first_head_kv: \u001b[38;5;28mbool\u001b[39m,\n\u001b[32m    490\u001b[39m ) -> torch.Tensor:\n\u001b[32m    491\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Attention computation.\u001b[39;00m\n\u001b[32m    492\u001b[39m \u001b[33;03m    Called by 'forward', potentially on shards, once shapes have been normalized.\u001b[39;00m\n\u001b[32m    493\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     q, k, v, kv, qkv = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_qkv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_kv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cached_kv\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cached_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreuse_first_head_kv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreuse_first_head_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m     attention_head_outputs = MultiHeadAttention.compute_attention_heads(\n\u001b[32m    505\u001b[39m         q,\n\u001b[32m    506\u001b[39m         k,\n\u001b[32m   (...)\u001b[39m\u001b[32m    511\u001b[39m         \u001b[38;5;28mself\u001b[39m.softmax_scale,\n\u001b[32m    512\u001b[39m     )\n\u001b[32m    513\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.einsum(\n\u001b[32m    514\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m... h d, h d s -> ... s\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    515\u001b[39m         attention_head_outputs,\n\u001b[32m    516\u001b[39m         \u001b[38;5;28mself\u001b[39m._w_out,\n\u001b[32m    517\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramData\\PythonEnv\\hull-tabpfn\\Lib\\site-packages\\tabpfn\\architectures\\base\\attention\\full_attention.py:443\u001b[39m, in \u001b[36mMultiHeadAttention.compute_qkv\u001b[39m\u001b[34m(self, x, x_kv, k_cache, v_cache, kv_cache, cache_kv, use_cached_kv, reuse_first_head_kv)\u001b[39m\n\u001b[32m    441\u001b[39m     orig_num_heads = w_kv.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    442\u001b[39m     w_kv = w_kv[:, :\u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m kv = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m... s, j h d s -> ... j h d\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_kv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_kv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reuse_first_head_kv:\n\u001b[32m    445\u001b[39m     expand_shape = [-\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m kv.shape]\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramData\\PythonEnv\\hull-tabpfn\\Lib\\site-packages\\torch\\functional.py:407\u001b[39m, in \u001b[36meinsum\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, *_operands)\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) <= \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum.enabled:\n\u001b[32m    405\u001b[39m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[32m    406\u001b[39m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    409\u001b[39m path = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum.is_available():\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "submission_df = run_pipeline(train_if_needed=True)\n",
        "elapsed = time.perf_counter() - t0\n",
        "print(f\"耗时：{elapsed:.2f} 秒\")\n",
        "submission_df.head()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
